{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPR - FACTORIZATION MACHINES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we train the BPR algorithm using the Factorization Machines as the underlying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from BPR_FM import BPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../categorized_offers/trips_combined_final.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been loaded, we need to filter the mobility requests based on the number of offers. More specifically, we study two different scenarios: requests with 100 offers or less and requests with 10 offers or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of offers of each mobility request\n",
    "counts = df.groupby('request_id').count().sort_values('offer_id').reset_index()[['request_id','offer_id']]\n",
    "\n",
    "# filter\n",
    "max_offers = 10\n",
    "user_ids_to_save = counts[counts['offer_id']<=max_offers]\n",
    "cleaned_df = pd.merge(df, user_ids_to_save['request_id'], on='request_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We intend to analyze the performance of the ranking algorithm depending on the number of trips registered by the users. For that, we will perform three different experiments:\n",
    "- Training with all the users\n",
    "- Training with users who have registered more than 20 trips\n",
    "- Training with users who have registered more than 40 trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users with more than 40 trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data based on number of trips registered by the users\n",
    "trips_users = 40\n",
    "trips_df = cleaned_df[cleaned_df['Response']==1]\n",
    "counts = trips_df.groupby('user_id').count().sort_values('request_id').reset_index()[['user_id','request_id']]\n",
    "trips_ids_to_save = counts[counts['request_id']>=trips_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users who registered more than 40: 46\n"
     ]
    }
   ],
   "source": [
    "print('Number of users who registered more than {}: {}'.format(trips_users, len(trips_ids_to_save)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataset\n",
    "df_to_train = pd.merge(cleaned_df, trips_ids_to_save['user_id'], on='user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is ready, we can train the algorithm. In this case, we train for 120k iterations. Several runs have been performed and after around this number of iterations the performance starts to decrease. The learning rate and regularizer have also been chosen based on some previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "2/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "3/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "4/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "5/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "6/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "7/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "8/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "9/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "10/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n"
     ]
    }
   ],
   "source": [
    "reco = BPR(df_to_train)\n",
    "reco.fit(single_iterations=150000, learning_rate=0.0005, lmbda=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the metrics for their further examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall at k test \n",
    "recall_at_1_test_avg = list() \n",
    "recall_at_5_test_avg = list()\n",
    "recall_at_10_test_avg = list()\n",
    "for i in range(len(reco.recall_at_k_test)):\n",
    "    average_1 = sum(reco.recall_at_k_test[i][1].values())/len(reco.recall_at_k_test[i][1])\n",
    "    recall_at_1_test_avg.append(average_1)\n",
    "    average_5 = sum(reco.recall_at_k_test[i][5].values())/len(reco.recall_at_k_test[i][5])\n",
    "    recall_at_5_test_avg.append(average_5)\n",
    "    average_10 = sum(reco.recall_at_k_test[i][10].values())/len(reco.recall_at_k_test[i][10])\n",
    "    recall_at_10_test_avg.append(average_10)\n",
    "\n",
    "recall_at_1_test_avg = np.concatenate([np.array(reco.metrics_iterations).reshape(-1,1),\n",
    "                                       np.array(recall_at_1_test_avg).reshape(-1,1),],axis=1)\n",
    "\n",
    "recall_at_5_test_avg = np.concatenate([np.array(reco.metrics_iterations).reshape(-1,1),\n",
    "                                       np.array(recall_at_5_test_avg).reshape(-1,1),],axis=1)\n",
    "\n",
    "recall_at_10_test_avg = np.concatenate([np.array(reco.metrics_iterations).reshape(-1,1),\n",
    "                                       np.array(recall_at_10_test_avg).reshape(-1,1),],axis=1)\n",
    "\n",
    "# save the averages\n",
    "np.savetxt('results_lt_10/results_users_gt_40/recall_testing/recall_at_1_test_u_gt_40.txt', recall_at_1_test_avg)\n",
    "np.savetxt('results_lt_10/results_users_gt_40/recall_testing/recall_at_5_test_u_gt_40.txt', recall_at_5_test_avg)\n",
    "np.savetxt('results_lt_10/results_users_gt_40/recall_testing/recall_at_10_test_u_gt_40.txt', recall_at_10_test_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP test \n",
    "MAP_test_avg = list() \n",
    "for i in range(len(reco.MAP_test)):\n",
    "    average = sum(reco.MAP_test[i].values())/len(reco.MAP_test[i])\n",
    "    MAP_test_avg.append(average)\n",
    "\n",
    "\n",
    "MAP_test_avg = np.concatenate([np.array(reco.metrics_iterations).reshape(-1,1),\n",
    "                               np.array(MAP_test_avg).reshape(-1,1),],axis=1)\n",
    "\n",
    "# save the averages\n",
    "np.savetxt('results_lt_10/results_users_gt_40/MAP_testing/MAP_test_u_gt_40.txt', MAP_test_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users with more than 20 trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data based on number of trips registered by the users\n",
    "trips_users = 20\n",
    "trips_df = cleaned_df[cleaned_df['Response']==1]\n",
    "counts = trips_df.groupby('user_id').count().sort_values('request_id').reset_index()[['user_id','request_id']]\n",
    "trips_ids_to_save = counts[counts['request_id']>=trips_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users who registered more than 20: 188\n"
     ]
    }
   ],
   "source": [
    "print('Number of users who registered more than {}: {}'.format(trips_users, len(trips_ids_to_save)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataset\n",
    "df_to_train = pd.merge(cleaned_df, trips_ids_to_save['user_id'], on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "2/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "3/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "4/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "5/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "6/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "7/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "8/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "9/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "10/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n"
     ]
    }
   ],
   "source": [
    "reco = BPR(df_to_train)\n",
    "reco.fit(single_iterations=150000, learning_rate=0.0005, lmbda=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall at k test \n",
    "recall_at_1_test_avg = list() \n",
    "recall_at_5_test_avg = list()\n",
    "recall_at_10_test_avg = list()\n",
    "for i in range(len(reco.recall_at_k_test)):\n",
    "    average_1 = sum(reco.recall_at_k_test[i][1].values())/len(reco.recall_at_k_test[i][1])\n",
    "    recall_at_1_test_avg.append(average_1)\n",
    "    average_5 = sum(reco.recall_at_k_test[i][5].values())/len(reco.recall_at_k_test[i][5])\n",
    "    recall_at_5_test_avg.append(average_5)\n",
    "    average_10 = sum(reco.recall_at_k_test[i][10].values())/len(reco.recall_at_k_test[i][10])\n",
    "    recall_at_10_test_avg.append(average_10)\n",
    "\n",
    "recall_at_1_test_avg = np.concatenate([np.array(reco.metrics_iterations).reshape(-1,1),\n",
    "                                       np.array(recall_at_1_test_avg).reshape(-1,1),],axis=1)\n",
    "\n",
    "recall_at_5_test_avg = np.concatenate([np.array(reco.metrics_iterations).reshape(-1,1),\n",
    "                                       np.array(recall_at_5_test_avg).reshape(-1,1),],axis=1)\n",
    "\n",
    "recall_at_10_test_avg = np.concatenate([np.array(reco.metrics_iterations).reshape(-1,1),\n",
    "                                       np.array(recall_at_10_test_avg).reshape(-1,1),],axis=1)\n",
    "\n",
    "# save the averages\n",
    "np.savetxt('results_lt_10/results_users_gt_20/recall_testing/recall_at_1_test_u_gt_20.txt', recall_at_1_test_avg)\n",
    "np.savetxt('results_lt_10/results_users_gt_20/recall_testing/recall_at_5_test_u_gt_20.txt', recall_at_5_test_avg)\n",
    "np.savetxt('results_lt_10/results_users_gt_20/recall_testing/recall_at_10_test_u_gt_20.txt', recall_at_10_test_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP test \n",
    "MAP_test_avg = list() \n",
    "for i in range(len(reco.MAP_test)):\n",
    "    average = sum(reco.MAP_test[i].values())/len(reco.MAP_test[i])\n",
    "    MAP_test_avg.append(average)\n",
    "\n",
    "\n",
    "MAP_test_avg = np.concatenate([np.array(reco.metrics_iterations).reshape(-1,1),\n",
    "                               np.array(MAP_test_avg).reshape(-1,1),],axis=1)\n",
    "\n",
    "# save the averages\n",
    "np.savetxt('results_lt_10/results_users_gt_20/MAP_testing/MAP_test_u_gt_20.txt', MAP_test_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data based on number of trips registered by the users\n",
    "trips_users = 0\n",
    "trips_df = cleaned_df[cleaned_df['Response']==1]\n",
    "counts = trips_df.groupby('user_id').count().sort_values('request_id').reset_index()[['user_id','request_id']]\n",
    "trips_ids_to_save = counts[counts['request_id']>=trips_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users who registered more than 0: 1517\n"
     ]
    }
   ],
   "source": [
    "print('Number of users who registered more than {}: {}'.format(trips_users, len(trips_ids_to_save)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataset\n",
    "df_to_train = pd.merge(cleaned_df, trips_ids_to_save['user_id'], on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "2/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "3/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "4/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "5/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "6/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "7/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "8/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "9/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n",
      "10/10\n",
      "Computing metrics...\n",
      "Metrics for train completed\n",
      "Metrics for test completed\n"
     ]
    }
   ],
   "source": [
    "reco = BPR(df_to_train)\n",
    "reco.fit(single_iterations=150000, learning_rate=0.0005, lmbda=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall at k test \n",
    "recall_at_1_test_avg = list() \n",
    "recall_at_5_test_avg = list()\n",
    "recall_at_10_test_avg = list()\n",
    "for i in range(len(reco.recall_at_k_test)):\n",
    "    average_1 = sum(reco.recall_at_k_test[i][1].values())/len(reco.recall_at_k_test[i][1])\n",
    "    recall_at_1_test_avg.append(average_1)\n",
    "    average_5 = sum(reco.recall_at_k_test[i][5].values())/len(reco.recall_at_k_test[i][5])\n",
    "    recall_at_5_test_avg.append(average_5)\n",
    "    average_10 = sum(reco.recall_at_k_test[i][10].values())/len(reco.recall_at_k_test[i][10])\n",
    "    recall_at_10_test_avg.append(average_10)\n",
    "\n",
    "recall_at_1_test_avg = np.concatenate([np.array(reco.metrics_iterations).reshape(-1,1),\n",
    "                                       np.array(recall_at_1_test_avg).reshape(-1,1),],axis=1)\n",
    "\n",
    "recall_at_5_test_avg = np.concatenate([np.array(reco.metrics_iterations).reshape(-1,1),\n",
    "                                       np.array(recall_at_5_test_avg).reshape(-1,1),],axis=1)\n",
    "\n",
    "recall_at_10_test_avg = np.concatenate([np.array(reco.metrics_iterations).reshape(-1,1),\n",
    "                                       np.array(recall_at_10_test_avg).reshape(-1,1),],axis=1)\n",
    "\n",
    "# save the averages\n",
    "np.savetxt('results_lt_10/results_users/recall_testing/recall_at_1_test_u.txt', recall_at_1_test_avg)\n",
    "np.savetxt('results_lt_10/results_users/recall_testing/recall_at_5_test_u.txt', recall_at_5_test_avg)\n",
    "np.savetxt('results_lt_10/results_users/recall_testing/recall_at_10_test_u.txt', recall_at_10_test_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP test \n",
    "MAP_test_avg = list() \n",
    "for i in range(len(reco.MAP_test)):\n",
    "    average = sum(reco.MAP_test[i].values())/len(reco.MAP_test[i])\n",
    "    MAP_test_avg.append(average)\n",
    "\n",
    "\n",
    "MAP_test_avg = np.concatenate([np.array(reco.metrics_iterations).reshape(-1,1),\n",
    "                               np.array(MAP_test_avg).reshape(-1,1),],axis=1)\n",
    "\n",
    "# save the averages\n",
    "np.savetxt('results_lt_10/results_users/MAP_testing/MAP_test_u.txt', MAP_test_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the variable ``max_offers`` to 100, we generate the results for the other case "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
